skip_initial_validation: False
output_path: ckpt/dit

# training hyperparams
epochs: 9
learning_rate: 0.001
momentum: 0.9
weight_decay: 0.0001 # 1e-4
optimizer: adam # must be either sgd or adam

# lr scheduling
lr_scheduler:
  type: cosine
  min_lr: 1.e-6
output_path: ckpt/dit

# model architecture
model:
  arch: dit  # resnet50, or resnet50ssl for semi-supervised resnet

  compression_parameters:
    ignored_modules:
      # list of layer names that you do not want to compress.
      # We follow Stock et al. "And the bit goes down: revisiting the quantization of deep neural networks", ICLR 2020
      # and do not compress the first 7x7 convolutional layer, as it represents only 0.1-0.05% of the network weights
      - x_embedder
      - t_embedder
      - y_embedder
      - final_layer
      # - blocks

    k: 256
    fc_subvector_size: 8 # d_fc
    pw_subvector_size: 4 # d_pw
    # Small or large block compression regime for convolutional layers
    large_subvectors: False
    k_means_type: src # kmeans, kmedians, src, slow_src
    # k_means_n_iters: 1_000  # Keep this low for fast iteration (eg during development). ~100 for decent results
    k_means_n_iters: 1_00 # Keep this low for fast iteration (eg during development). ~100 for decent results
    # k_means_n_iters: 1_000 # Keep this low for fast iteration (eg during development). ~100 for decent results

    # Used to overwrite configs
    layer_specs:
      fc:
        k: 1024  # same as BGD
        k_means_type: src

  sls_iterations: 10_000